{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Memories in Multi-Turn Agent Conversations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent research from Salesforce AI Research found that: [\"LLMs Get Lost In Multi-Turn Conversation\"](https://arxiv.org/pdf/2505.06120):\n",
    "\n",
    "> \"Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that \\*when LLMs take a wrong turn in a conversation, they get lost and do not recover.\"\n",
    "\n",
    "To help avoid this, we can implement a custom short-term and long-term memory to ensure that the conversation turns never get too long, and condense the memory as we go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this work, we need two things\n",
    "\n",
    "1.  A memory block that condenses a;; past chat messages into a single string while maintaining a token limit\n",
    "2.  A `Memory` instance that uses that memory block, and has token limits configured such that multi-turn conversations are always flushed to the memory block for handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Memory Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our custom made memory block -- and we significantly reduced the amount of non-essential characters which often clog tool call outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from pydantic import Field\n",
    "from typing import List, Optional, Any\n",
    "from llama_index.core.llms import ChatMessage, TextBlock\n",
    "from llama_index.core.memory import Memory, BaseMemoryBlock\n",
    "\n",
    "\n",
    "class CondensedMemoryBlock(BaseMemoryBlock[str]):\n",
    "    \"\"\"\n",
    "    This class is a smart conversation buffer that maintains context while \n",
    "    staying within reasonable memory limits.\n",
    "\n",
    "    It condenses the conversation history into a single string, while \n",
    "    maintaining a token limit.\n",
    "\n",
    "    It also includes additional kwargs, like tool calls, when needed.\n",
    "    \"\"\"\n",
    "    current_memory: List[str] = Field(default_factory=list)\n",
    "    token_limit: int = Field(default=50000)\n",
    "    tokenizer: tiktoken.Encoding = tiktoken.encoding_for_model(\"gpt-4o\") \n",
    "\n",
    "    async def _aget(\n",
    "        self, messages: Optional[List[ChatMessage]] = None, **block_kwargs: Any\n",
    "    ) -> str:\n",
    "        \"\"\"Return the current memory block contents.\"\"\"\n",
    "        return \"\\n\".join(self.current_memory)\n",
    "\n",
    "    async def _aput(self, messages: List[ChatMessage]) -> None:\n",
    "        \"\"\"Push messages into the memory block. (Only handles text content)\"\"\"\n",
    "        # construct a string for each message\n",
    "        for message in messages:\n",
    "            text_contents = \"\\n\".join(\n",
    "                block.text\n",
    "                for block in message.blocks\n",
    "                if isinstance(block, TextBlock)\n",
    "            )\n",
    "            memory_str = text_contents if text_contents else \"\"\n",
    "            kwargs = {}\n",
    "            for key, val in message.additional_kwargs.items():\n",
    "                if key == \"tool_calls\":\n",
    "                    val = [\n",
    "                        {\n",
    "                            \"name\": tool_call[\"function\"][\"name\"],\n",
    "                            \"args\": tool_call[\"function\"][\"arguments\"],\n",
    "                        }\n",
    "                        for tool_call in val\n",
    "                    ]\n",
    "                    kwargs[key] = val\n",
    "                elif key != \"session_id\" and key != \"tool_call_id\":\n",
    "                    kwargs[key] = val\n",
    "            memory_str += f\"\\n({kwargs})\" if kwargs else \"\"\n",
    "\n",
    "            self.current_memory.append(memory_str)\n",
    "\n",
    "        # ensure this memory block doesn't get too large\n",
    "        message_length = sum(\n",
    "            len(self.tokenizer.encode(message))\n",
    "            for message in self.current_memory\n",
    "        )\n",
    "        while message_length > self.token_limit:\n",
    "            self.current_memory = self.current_memory[1:]\n",
    "            message_length = sum(\n",
    "                len(self.tokenizer.encode(message))\n",
    "                for message in self.current_memory\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, a `Memory` instance that uses that block while configuring a very limited token limit for the short-term memory:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Custom Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import Memory, InsertMethod\n",
    "\n",
    "block = CondensedMemoryBlock(name=\"condensed_memory\")\n",
    "\n",
    "memory = Memory.from_defaults(\n",
    "    session_id=\"summary_memory\",\n",
    "    token_limit=60000,\n",
    "    token_flush_size=5000,\n",
    "    async_database_uri=\"sqlite+aiosqlite:///:memory:\",\n",
    "    memory_blocks=[block],\n",
    "    insert_method=InsertMethod.USER,\n",
    "    chat_history_token_ratio=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello! My name is Megan\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Hello! How can I help you?\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is the capital of France?\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"The capital of France is Paris\"),\n",
    "]\n",
    "\n",
    "await memory.aput_messages(initial_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "await memory.aput_messages(\n",
    "    [ChatMessage(role=\"user\", content=\"What was my name again?\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> role: MessageRole.USER: <memory>\n",
      "<condensed_memory>\n",
      "Hello! My name is Megan\n",
      "Hello! How can I help you?\n",
      "What is the capital of France?\n",
      "The capital of France is Paris\n",
      "</condensed_memory>\n",
      "</memory>\n",
      "What was my name again?\n"
     ]
    }
   ],
   "source": [
    "chat_history = await memory.aget()\n",
    "\n",
    "for message in chat_history:\n",
    "    print(f\"=> role: {message.role}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tool Call Agent Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=[multiply, divide, add, subtract],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can do simple math operations with tools.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = CondensedMemoryBlock(name=\"condensed_memory\")\n",
    "\n",
    "memory = Memory.from_defaults(\n",
    "    session_id=\"tight-memory\",\n",
    "    token_limit=60000,\n",
    "    token_flush_size=5000,\n",
    "    async_database_uri=\"sqlite+aiosqlite:///:memory:\",\n",
    "    memory_blocks=[block],\n",
    "    insert_method=\"user\",\n",
    "    chat_history_token_ratio=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of (3214 * 322) / 2 is 517454.0.\n"
     ]
    }
   ],
   "source": [
    "resp = await agent.run(\"What is (3214 * 322) / 2?\", memory=memory)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> role: MessageRole.ASSISTANT: The result of (3214 * 322) / 2 is 517454.0.\n",
      "=> role: MessageRole.USER: <memory>\n",
      "<condensed_memory>\n",
      "What is (3214 * 322) / 2?\n",
      "\n",
      "({'tool_calls': [{'name': 'multiply', 'args': '{\"a\": 3214, \"b\": 322}'}, {'name': 'divide', 'args': '{\"a\": 3214, \"b\": 2}'}]})\n",
      "1034908\n",
      "1607.0\n",
      "\n",
      "({'tool_calls': [{'name': 'divide', 'args': '{\"a\":1034908,\"b\":2}'}]})\n",
      "517454.0\n",
      "</condensed_memory>\n",
      "</memory>\n"
     ]
    }
   ],
   "source": [
    "chat_history = await memory.aget()\n",
    "\n",
    "for message in chat_history:\n",
    "    print(f\"=> role: {message.role}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last question you asked was: \"What is (3214 * 322) / 2?\"\n"
     ]
    }
   ],
   "source": [
    "resp = await agent.run(\n",
    "    \"What was the last question I asked you?\", memory=memory\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer the question \"(3214 * 322) / 2,\" I first multiplied 3214 by 322 to get the product. Then, I divided that product by 2 to get the final result. Specifically:\n",
      "\n",
      "1. Multiply 3214 by 322.\n",
      "2. Divide the result by 2.\n",
      "\n",
      "The final answer is 517454.0.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = await agent.run(\n",
    "    \"And how did you go about answering that message?\", memory=memory\n",
    ")\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
